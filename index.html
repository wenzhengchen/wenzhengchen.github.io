<!DOCTYPE html>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="stylesheet" type="text/css" href="stylesheet.css" />
    <!-- <link rel="icon" type="image/png" href="images/none.png" /> -->
    <title>Wenzheng Chen</title>
    <!-- <meta name="author" content="Wenzheng Chen" /> -->
</head>

<body>
    <table class="main-table">
        <tbody>
            <tr style="padding: 0px">
                <td style="padding: 0px">
                    <!-- Bio ---------------------------------->
                    <table class="content-table">
                        <tbody>
                            <tr style="padding: 0px">
                                <td style="padding: 2.5%; width: 63%; vertical-align: middle">
                                    <p style="text-align: center">
                                        <name>Wenzheng Chen</name>
                                    </p>
                                    <p style="justify-content: space-between">
                                        I'm a tenure-track Assistant Professor at <a
                                            href="https://www.wict.pku.edu.cn/xztd/xztd_01/1222604.htm">Wangxuan
                                            Institute of Computer Technology</a>, <a
                                            href="https://www.pku.edu.cn/">Peking University</a>, where I mainly work on
                                        Computational Photography and 3D Vision.
                                        I am also affiliated in the <a href="https://vcl.pku.edu.cn/">Visual Computing
                                            and Learning Lab</a>, collaborating closely with <a
                                            href="https://cfcs.pku.edu.cn/baoquan/">Prof. Baoquan Chen</a>, <a
                                            href=http://libliu.info />Prof. Libin Liu</a>, <a
                                            href="https://rachelcmy.github.io/">Prof. Mengyu Chu</a>, and <a
                                            href="https://wang-ps.github.io/">Prof. Pengshuai Wang</a>.
                                    </p>
                                    <p style="justify-content: space-between">
                                        Before joining Peking University, I was a research scientist at
                                        <a href="https://research.nvidia.com/labs/toronto-ai/">NVIDIA Toronto AI
                                            Lab</a>.
                                        I earned my Ph.D. from the University of Toronto and received both my Master's
                                        and Bachelor's degrees from Shandong University.</a>
                                    </p>
                                    <p style="justify-content: space-between">
                                        I am always actively recruiting Ph.D. students and research interns! Feel free
                                        to drop me a line with your CV and research statement!
                                    </p>
                                    <p style="text-align: center">
                                        <a href="mailto:wenzhengchen@pku.edu.cn">Email</a>
                                        &nbsp/&nbsp
                                        <a href="wenzheng-cv-2025.07.pdf">CV</a>
                                        &nbsp/&nbsp
                                        <a href="https://scholar.google.com/citations?user=KzhR_TsAAAAJ&hl=en">Google
                                            Scholar</a>
                                        &nbsp/&nbsp
                                        <a href="https://www.linkedin.com/in/wenzheng-chen-700823116/">
                                            LinkedIn
                                        </a>
                                    </p>
                                </td>
                                <td style="padding: 2.5%; width: 40%; max-width: 40%">
                                    <a href="images/7341614762140.jpg"><img style="width: 100%; max-width: 100%"
                                            alt="profile photo" src="images/7341614762140.jpg"
                                            class="hoverZoomLink" /></a>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <!-- Reserch -->
                    <table>
                        <tr>
                            <td style="padding: 20px 0; width: 100%; vertical-align: middle">
                                <div class="info-box">
                                    <heading style="margin-bottom: 10px; display:block;">Research</heading>
                                    <p>
                                        My research focuses on <strong>Computational Photography</strong> and <strong>3D
                                            Vision</strong>.
                                    </p>
                                    <p>
                                        My goal is to integrate <strong>3D Sensing</strong> with <strong>World
                                            Models</strong> to enable intelligent agents to perceive, simulate, and
                                        interact with the physical world.
                                    </p>
                                </div>
                            </td>
                        </tr>
                    </table>

                    <!-- News -->
                    <table class="content-table">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>News</heading>
                                    <p>
                                    <div class="news-item"><span class="news-date">[2025.12]</span> One paper accepted
                                        to
                                        <strong><a href="https://ai4scientificimaging.org/dive3d/">TMLR</a></strong>. We
                                        solve the mode collapse problem by replacing KL with SIM loss.
                                    </div>

                                    <div class="news-item"><span class="news-date">[2025.12]</span> One paper accepted
                                        to
                                        <strong><a href="https://ai4scientificimaging.org/latentdem/">TIP</a></strong>.
                                        We tackle the challenging blind inverse problems with latent diffusion priors.
                                    </div>

                                    <!---<div class="news-item"><span class="news-date">[2025.11]</span> One paper accepted to <strong>3DV
                                            2026</strong>. We propose an inverse rendering technique for extremely motion-blurred 3D reconstrucion.
                                    </div>
                                    -->

                                    <div class="news-item"><span class="news-date">[2025.08]</span> One paper accepted
                                        to <strong><a href="https://namisntimpot.github.io/NSLweb/">SIGGRAPH
                                                Asia 2025</a></strong>. We developed a
                                        powerful structured light 3D imaging technique achieving 10x accuracy
                                        improvement over traditional methods.</div>

                                    <div id="moreNews" style="display:none">

                                        <div class="news-item"><span class="news-date">[2025.03]</span> One paper
                                            accepted to <strong>ICCV
                                                2025</strong>. Check out <a
                                                href="https://pku-vcl-geometry.github.io/GeoSplatting/">GeoSplatting</a>:
                                            Geometry-guided
                                            Gaussian Splatting!</div>

                                        <div class="news-item"><span class="news-date">[2025.03]</span> Serving as Area
                                            Chair for
                                            <strong>NeurIPS 2025</strong>.
                                        </div>

                                        <div class="news-item"><span class="news-date">[2025.02]</span> Two papers
                                            accepted to <strong>CVPR
                                                2025</strong>. Check our <a
                                                href="https://pku-vcl-geometry.github.io/RainyGS/">Physically-based Rain
                                                Synthesis</a> work!
                                        </div>

                                    </div>

                                    <a href="javascript:void(0)" onclick="toggleNews()" id="toggleNewsBtn"
                                        style="font-size: small; color: #0066cc; text-decoration: none;">[ more... ]</a>
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <!-- Hiring -->
                    <table class="content-table">
                        <tbody>
                            <tr>
                                <td style="padding: 20px 0; width: 100%; vertical-align: middle">
                                    <div class="info-box">
                                        <heading style="margin-bottom: 10px; display:block;">üî• Hiring / ÊãõÁîü</heading>
                                        <p>
                                            I am actively recruiting <b>Ph.D. students (2027 intake)</b> and <b>Research
                                                Interns (longer
                                                than 3 months)</b>.
                                        </p>
                                        <p>
                                            If you are interested, please send your CV and transcript to <a
                                                href="mailto:wenzhengchen@pku.edu.cn">wenzhengchen@pku.edu.cn</a>.
                                        </p>
                                        <p style="font-size: 0.9em; color: #555;">
                                            (ÂÆûÈ™åÂÆ§ÈùûÂ∏∏Ê¨¢ËøéÊ†°ÂÜÖÂ§ñÊú¨ÁßëÁîü„ÄÅÁ†îÁ©∂ÁîüÂêåÂ≠¶Êù•ÂâçÊù•ÂÆû‰π†ÊàñËÆøÈóÆÔºåËØ∑ÊÑüÂÖ¥Ë∂£ÁöÑÂêåÂ≠¶Áõ¥Êé•ÂíåÊàëËÅîÁ≥ª„ÄÇ)
                                        </p>
                                    </div>
                                </td>
                            </tr>
                        </tbody>
                    </table>


                    <!-- Publication head-->
                    <table class="content-table">
                        <tbody>
                            <tr>
                                <td style="padding: 20px; width: 100%; vertical-align: middle">
                                    <heading>Selected Publications</heading>
                                    <!--
                  <p>Representative papers are <span class="highlight">highlighted</span>, with full publication list in
                    <a href="https://scholar.google.com/citations?user=KzhR_TsAAAAJ&hl=en">Google Scholar</a>.
                    -->
                                    <p>Full publication list in
                                        <a href="https://scholar.google.com/citations?user=KzhR_TsAAAAJ&hl=en">Google
                                            Scholar</a>.
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <!-- Publication head-->
                    <table class="content-table">
                        <tbody>

                            <!-- Year 2026 -->
                            <tr>
                                <td class="year-divider" colspan="2">
                                    <span class="year-label">2026</span>
                                </td>
                            </tr>

                            <!-- publication 2026-->

                            <tr onmouseout="" onmouseover="">
                                <td style="padding: 20px; width: 25%; vertical-align: middle">
                                    <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                                        <source src="images/2026-orbit.mp4" type="video/mp4">
                                    </video>
                                </td>
                                <td style="padding: 20px; width: 75%; vertical-align: middle">
                                    <a href="https://pku-vcl-geometry.github.io/Orbit2Ground/">
                                        <papertitle>From Orbit to Ground: Generative City Photogrammetry from Extreme
                                            Off-Nadir Satellite Images
                                        </papertitle>
                                    </a>
                                    <br>
                                    <a href="https://maxmilite.github.io/">Fei Yu*</a>,
                                    <a href="https://scholar.google.com/citations?user=XchgHFoAAAAJ">Yu Liu*</a>,
                                    <a href="https://scholar.google.com/citations?user=2A1sp-wAAAAJ">Luyang Tang</a>,
                                    <a href="https://scholar.google.com/citations?user=bqsITKQAAAAJ">Mingchao Sun</a>,
                                    <a href="https://github.com/boolwater">Zengye Ge</a>,
                                    <a href="https://scholar.google.com/citations?user=YMRzlkMAAAAJ">Rui Bu</a>,
                                    <br>
                                    <a href="https://nobodywant2try.github.io/">Yuchao Jin</a>,
                                    <a href="https://haisenzhao.github.io/">Haisen Zhao</a>,
                                    <a href="https://ai4scientificimaging.org/">He Sun</a>,
                                    <a href="https://yangyan.li/">Yangyan Li</a>,
                                    <a href="https://scholar.google.com/citations?user=JGi4S0EAAAAJ">Mu
                                        Xu<sup>#</sup></a>,
                                    <strong>Wenzheng Chen<sup>#</sup></strong>,
                                    <a href="http://baoquanchen.info/">Baoquan Chen<sup>#</sup></a>
                                    <br>
                                    <em>arXiv</em>, 2025
                                    <br>
                                    <a href="https://pku-vcl-geometry.github.io/Orbit2Ground/">project page</a>
                                    /
                                    <a href="https://arxiv.org/abs/2512.07527">arXiv</a>
                                    /
                                    <a href="./bib/26-orbit.bib">bibtex</a>
                                    <p></p>
                                    <p>
                                        We propose a method to enable photorealistic city-scale 3D reconstruction from
                                        extreme off-nadir satellite
                                        imagery.
                                    </p>
                                </td>
                            </tr>

                            <!-- Year 2025 -->
                            <tr>
                                <td class="year-divider" colspan="2">
                                    <span class="year-label">2025</span>
                                </td>
                            </tr>

                            <!-- Publication 2025-->

                            <tr onmouseout="nsl2026_stop()" onmouseover="nsl2026_start()">
                                <td style="padding: 20px; width: 25%; vertical-align: middle">
                                    <img width="100%" src="images/2025-nsl2.gif" />
                                </td>
                                <td style="padding: 20px; width: 75%; vertical-align: middle">
                                    <a href="https://namisntimpot.github.io/NSLweb/">
                                        <papertitle>Robust Single-shot Structured Light 3D Imaging via Neural Feature
                                            Decoding</papertitle>
                                    </a>
                                    <br>
                                    <a href="https://namisntimpot.github.io/">Jiaheng Li*</a>,
                                    <a href="https://daiqy.github.io/">Qiyu Dai*</a>,
                                    <a href="https://www.linkedin.com/in/lihan-lilyan-li-8684362aa/">Lihan Li</a>,
                                    <a href=https://www.cs.unc.edu/~cpk />Praneeth Chakravarthula</a>,
                                    <a href=https://ai4scientificimaging.org/#/team>He Sun</a>,
                                    <a href="https://baoquanchen.info/">Baoquan Chen<sup>#</sup></a>,
                                    <strong>Wenzheng Chen<sup>#</sup></strong>
                                    <br>
                                    <em>SIGGRAPH Asia</em>, 2025
                                    <br>
                                    <a href="https://namisntimpot.github.io/NSLweb/">project page</a>
                                    /
                                    <a href="https://arxiv.org/abs/2512.14028">arXiv</a>
                                    /
                                    <a href="https://github.com/Namisntimpot/NSL">code</a>
                                    /
                                    <a href="./bib/25-nsl.bib">bibtex</a>
                                    <p></p>
                                    <p> Neural single-shot Structured Light (NSL) achieves robust and high-fidelity 3D
                                        reconstruction from a single-shot structured light input.</p>
                                </td>
                            </tr>

                            <tr onmouseout="geosplatting_stop()" onmouseover="geosplatting_start()">
                                <td style="padding: 20px; width: 25%; vertical-align: middle">
                                    <img width="90%" src="images/2025-geosplatting.png" />
                                </td>
                                <td style="padding: 20px; width: 75%; vertical-align: middle">
                                    <a href="https://pku-vcl-geometry.github.io/GeoSplatting/">
                                        <papertitle>GeoSplatting: Towards Geometry Guided Gaussian Splatting for
                                            Physically-based Inverse Rendering</papertitle>
                                    </a>
                                    <br>
                                    <a href=https://illusive-chase.github.io />Kai Ye*</a>,
                                    <a href=https://timchonggao.github.io/ChongGao.github.io />Chong Gao*</a>,
                                    <a href=https://guanbinli.com />Guanbin Li</a>,
                                    <strong>Wenzheng Chen<sup>#</sup></strong>,
                                    <a href="https://baoquanchen.info/">Baoquan Chen<sup>#</sup></a>
                                    <br>
                                    <em>ICCV</em>, 2025
                                    <br>
                                    <a href="https://pku-vcl-geometry.github.io/GeoSplatting/">project page</a>
                                    /
                                    <a href="https://arxiv.org/abs/2410.24204">arXiv</a>
                                    /
                                    <a href="https://github.com/PKU-VCL-Geometry/geosplatting">code (coming soon)</a>
                                    /
                                    <a href="./bib/25-geosplatting.bib">bibtex</a>
                                    <p></p>
                                    <p> GeoSplatting introduces a novel hybrid representation that grounds 3DGS with
                                        isosurfacing to provide accurate geometry and normals for high-fidelity inverse
                                        rendering. </p>
                                </td>
                            </tr>


                            <tr onmouseout="rainygs_stop()" onmouseover="rainygs_start()">
                                <td style="padding: 20px; width: 25%; vertical-align: middle">
                                    <img width="90%" src="images/2025_RainyGS-2.gif" />
                                </td>
                                <td style="padding: 20px; width: 75%; vertical-align: middle">
                                    <a href="https://pku-vcl-geometry.github.io/RainyGS/">
                                        <papertitle>RainyGS: Efficient Rain Synthesis with Physically-Based Gaussian
                                            Splatting</papertitle>
                                    </a>
                                    <br>
                                    <a href=https://daiqy.github.io />Qiyu Dai*</a>,
                                    <a href=https://starryuniv.cn />Xingyu Ni*</a>,
                                    <a href=https://qianfanshen.github.io />Qianfan Shen</a>,
                                    <strong>Wenzheng Chen<sup>#</sup></strong>,
                                    <a href="https://baoquanchen.info/">Baoquan Chen<sup>#</sup></a>,
                                    <a href="https://rachelcmy.github.io/">Mengyu Chu<sup>#</sup></a>
                                    <br>
                                    <em>CVPR</em>, 2025
                                    <br>
                                    <a href="https://pku-vcl-geometry.github.io/RainyGS/">project page</a>
                                    /
                                    <a href="https://arxiv.org/abs/2503.21442">arXiv</a>
                                    /
                                    <a href="https://github.com/PKU-VCL-Geometry/rainygs">code (coming soon)</a>
                                    /
                                    <a href="https://www.youtube.com/watch?v=FeuqlB7T_eQ">video</a>
                                    /
                                    <a href="./bib/25-rainygs.bib">bibtex</a>
                                    <p></p>
                                    <p> RainyGS integrates physics simulation with 3DGS to efficiently generate
                                        photorealistic, physically accurate, and controllable dynamic rain effects for
                                        in-the-wild scenes. </p>
                                </td>
                            </tr>

                            <!-- Year 2024 -->
                            <tr>
                                <td class="year-divider" colspan="2">
                                    <span class="year-label">2024</span>
                                </td>
                            </tr>

                            <!-- Publication 2024-->

                            <tr onmouseout="emdifusiion_stop()" onmouseover="emdiffusion_start()">
                                <td style="padding: 20px; width: 25%; vertical-align: middle">
                                    <img width="90%" src="images/24-emd.jpg" />
                                </td>
                                <td style="padding: 20px; width: 75%; vertical-align: middle">
                                    <a href="https://github.com/weiminbai/EMDiffusion">
                                        <papertitle>An Expectation-Maximization Algorithm for Training Clean Diffusion
                                            Models from Corrupted Observations</papertitle>
                                    </a>
                                    <br>
                                    <a>Yifei Wang*</a>,
                                    <a>Weimin Bai*</a>,
                                    <strong>Wenzheng Chen</strong>,
                                    <a href="https://hesunpu.github.io/">He Sun</a>
                                    (* Equal contribution)
                                    <br>
                                    <em>NeurIPS</em>, 2024
                                    <br>
                                    <a href="https://github.com/weiminbai/EMDiffusion">project page</a>
                                    /
                                    <a href="https://arxiv.org/abs/2407.01014">arXiv</a>
                                    /
                                    <a href="https://github.com/weiminbai/EMDiffusion">code</a>
                                    /
                                    <a href="./papers/24-emd.bib">bibtex</a>
                                    <p></p>
                                    <p> EMDiffusion learns a clean diffusion model from corrupted data. </p>
                                </td>
                            </tr>



                            <tr onmouseout="turbosl_stop()" onmouseover="turbosl_start()">
                                <td style="padding: 20px; width: 25%; vertical-align: middle">
                                    <img width="90%" src="images/24-turbosl.jpg" />
                                </td>
                                <td style="padding: 20px; width: 75%; vertical-align: middle">
                                    <a href="https://www.dgp.toronto.edu/turbosl">
                                        <papertitle>TurboSL: Dense, Accurate and Fast 3D by Neural Inverse Structured
                                            Light</papertitle>
                                    </a>
                                    <br>
                                    <a href=https://www.cs.toronto.edu/~parsa />Parsa Mirdehghan</a>, <a>Maxx Wu</a>,
                                    <strong>Wenzheng Chen</strong>,
                                    <a href="https://davidlindell.com/">David B. Lindell</a>,
                                    <a href="https://www.cs.toronto.edu/~kyros/">Kiriakos N. Kutulakos</a>
                                    <br>
                                    <em>CVPR</em>, 2024
                                    <br>
                                    <a href="https://www.dgp.toronto.edu/turbosl">project page</a>
                                    /
                                    <a
                                        href="https://www.dgp.toronto.edu/turbosl/static/data/TurboSL-paper.pdf">paper</a>
                                    /
                                    <a href="https://youtu.be/apc6l5KseFc">video</a>
                                    /
                                    <a href="https://www.dgp.toronto.edu/turbosl">code (coming soon)</a>
                                    /
                                    <a href="./papers/24-sl.bib">bibtex</a>
                                    <p></p>
                                    <p> TurboSL provides sub-pixel-accurate surfaces and normals at mega-pixel
                                        resolution from structured light images, captured at fractions of a second. </p>
                                </td>
                            </tr>


                            <tr onmouseout="gaussian_stop()" onmouseover="gaussian_start()">
                                <td style="padding: 20px; width: 25%; vertical-align: middle">
                                    <img width="100%" src="./images/24-gaussianflame.gif" />
                                </td>
                                <td style="padding: 20px; width: 75%; vertical-align: middle">
                                    <a href="https://weify627.github.io/4drotorgs/">
                                        <papertitle>4D-Rotor Gaussian Splatting: Towards Efficient Novel-View Synthesis
                                            for Dynamic Scenes
                                        </papertitle>
                                    </a>
                                    <br>
                                    <a href="https://github.com/LRLVEC">Yuanxing Duan*</a>,
                                    <a href="https://weify627.github.io/">Fangyin Wei*</a>,
                                    <a href="https://daiqy.github.io/">Qiyu Dai</a>,
                                    <a>Yuhang He</a>,
                                    <strong>Wenzheng Chen<sup>#</sup></strong>,
                                    <a href="https://cfcs.pku.edu.cn/baoquan/">Baoquan Chen<sup>#</sup></a>
                                    <br>
                                    (* Equal contribution, <sup>#</sup> joint corresponding authors)
                                    <br>
                                    <em>Proc. SIGGRAPH</em>, 2024
                                    <br>
                                    <a href="https://weify627.github.io/4drotorgs/">project page</a>
                                    /
                                    <a href="https://arxiv.org/abs/2402.03307">arXiv</a>
                                    /
                                    <a href="https://github.com/weify627/4D-Rotor-Gaussians">code</a>
                                    /
                                    <a href="./papers/24-gaussian.bib">bibtex</a>
                                </td>
                            </tr>

                            <!-- Year Before 2023 -->
                            <tr>
                                <td class="year-divider" colspan="2">
                                    <span class="year-label">2023 and before</span>
                                </td>
                            </tr>

                            <tr>
                                <td width="25%">
                                    <img src='./uoft.svg.png' width="100%">
                                </td>
                                <td valign="middle" width="75%">
                                    <a href="./Chen_Wenzheng_PHD_thesis.pdf">
                                        <papertitle>Boosting 3D Reconstruction with Differentiable Imaging Systems
                                        </papertitle>
                                    </a>
                                    <br>
                                    <strong>Wenzheng Chen</strong>
                                    <br>
                                    <em>Ph.D. Thesis, 2023</em>
                                    <br>
                                    <p>
                                    </p>
                                </td>
                            </tr>

                            <tr onmouseout="flexicubes_stop()" onmouseover="flexicubes_start()">
                                <td width="25%">
                                    <div class="one">
                                        <div class="two" id='flexicubes_image'>
                                            <img src='./images/flexicubes/dynamic.gif' width="100%">
                                        </div>
                                        <img src='./images/flexicubes/static.jpg' width="100%">
                                    </div>
                                    <script type="text/javascript">
                                        function flexicubes_start() {
                                            document.getElementById('flexicubes_image').style.opacity = "1";
                                        }

                                        function flexicubes_stop() {
                                            document.getElementById('flexicubes_image').style.opacity = "0";
                                        }
                                        flexicubes_stop()
                                    </script>
                                </td>
                                <td valign="middle" width="75%">
                                    <a href="https://research.nvidia.com/labs/toronto-ai/flexicubes/">
                                        <papertitle>Flexible Isosurface Extraction for Gradient-Based Mesh Optimization
                                        </papertitle>
                                    </a>
                                    <br>
                                    <a href="http://www.cs.toronto.edu/~shenti11/">Tianchang Shen</a>,
                                    <a href="https://research.nvidia.com/person/jacob-munkberg">Jacob Munkberg</a>,
                                    <a href="https://research.nvidia.com/person/jon-hasselgren">Jon Hasselgren</a>,
                                    <a href="https://kangxue.org/">Kangxue Yin</a>,
                                    <a href="http://www.cs.toronto.edu/~zianwang/">Zian Wang</a>,
                                    <strong>Wenzheng Chen</strong>,
                                    <a href="https://zgojcic.github.io/">Zan Gojcic</a>,
                                    <a href="http://www.cs.utoronto.ca/~fidler/">Sanja Fidler</a>,
                                    <a href="https://nmwsharp.com/">Nicholas Sharp*</a>,
                                    <a href="http://www.cs.toronto.edu/~jungao/">Jun Gao*</a>
                                    <br>
                                    <em>ACM Transactions on Graphics (SIGGRAPH), 2023</em>
                                    <br>
                                    <a href="https://research.nvidia.com/labs/toronto-ai/flexicubes/">project page</a> /
                                    <a href="https://arxiv.org/abs/2308.05371">arXiv</a> /
                                    <a href="https://github.com/nv-tlabs/FlexiCubes">code</a> /
                                    <a href="https://www.youtube.com/watch?v=V7kFK7aNCXI">video</a> /
                                    <a
                                        href="https://nv-tlabs.github.io/flexicubes_website/flexicubes_bib.txt">bibtex</a>
                                    <p>
                                    </p>
                                </td>
                            </tr>

                            <!-- Publication 2023-->
                            <tr onmouseout="fegr_stop()" onmouseover="fegr_start()">
                                <td style="padding: 20px; width: 25%; vertical-align: middle">
                                    <img width="100%" src="./images/fegr/output.gif" />
                                </td>

                                <td style="padding: 20px; width: 75%; vertical-align: middle">
                                    <a href="https://nv-tlabs.github.io/fegr/">
                                        <papertitle>Neural Fields meet Explicit Geometric Representations for Inverse
                                            Rendering of Urban
                                            Scenes</papertitle>
                                    </a>
                                    <br>
                                    <a href="http://www.cs.toronto.edu/~zianwang/">Zian Wang</a>,
                                    <a href="http://www.cs.toronto.edu/~shenti11/">Tianchang Shen</a>,
                                    <a href="http://www.cs.toronto.edu/~jungao/">Jun Gao</a>,
                                    <a href="https://shengyuh.github.io">Shengyu Huang</a>,
                                    <a href="https://research.nvidia.com/person/jacob-munkberg">Jacob Munkberg</a>,
                                    <a href="https://research.nvidia.com/person/jon-hasselgren">Jon Hasselgren</a>,
                                    <a href="https://zgojcic.github.io/">Zan Gojcic</a>,
                                    <strong>Wenzheng Chen</strong>,
                                    <a href="http://www.cs.utoronto.ca/~fidler/">Sanja Fidler</a>
                                    <br>
                                    <em>CVPR</em>, 2023
                                    <br>
                                    <a href="https://nv-tlabs.github.io/fegr/">project page</a> /
                                    <a href="https://arxiv.org/abs/2304.03266">arXiv</a> /
                                    code /
                                    <a href="https://www.youtube.com/watch?v=1KvHY3tlhhY">video</a> /
                                    <a href="https://nv-tlabs.github.io/fegr/assets/fegr_bib.txt">bibtex</a>
                                    <p>
                                        Combined with other NVIDIA technology, FEGR is one component of <a
                                            href="https://www.youtube.com/watch?v=vgot-CK1xRk">Neural Reconstruction
                                            Engine</a> announced in
                                        <a href="https://youtu.be/PWcNlRI00jo?t=2473" style="color:#FF0000;">GTC Sept
                                            2022 Keynote</a>.
                                    </p>
                                </td>
                            </tr>
                            <!-- Publication 2022-->
                            <tr onmouseout="get3d_stop()" onmouseover="get3d_start()">
                                <td style="padding: 20px; width: 25%; vertical-align: middle">
                                    <div class="one">
                                        <div class="two" id='get3d_image'>
                                            <img src='./images/get3d/get3d.gif' width="100%">
                                        </div>
                                        <img src='./images/get3d/get3d.jpeg' width="100%">
                                    </div>
                                    <script type="text/javascript">
                                        function get3d_start() {
                                            document.getElementById('get3d_image').style.opacity = "1";
                                        }

                                        function get3d_stop() {
                                            document.getElementById('get3d_image').style.opacity = "0";
                                        }
                                        get3d_stop()
                                    </script>
                                </td>
                                <td style="padding: 20px; width: 75%; vertical-align: top">
                                    <a href="https://nv-tlabs.github.io/GET3D/">
                                        <papertitle>GET3D: A Generative Model of High Quality 3D Textured Shapes Learned
                                            from Images
                                        </papertitle>
                                    </a>
                                    <br>
                                    <a href="http://www.cs.toronto.edu/~jungao/">Jun Gao</a>,
                                    <a href="http://www.cs.toronto.edu/~shenti11/">Tianchang Shen</a>,
                                    <a href="http://www.cs.toronto.edu/~zianwang/">Zian Wang</a>,
                                    <strong>Wenzheng Chen</strong>,
                                    <a href="https://kangxue.org/">Kangxue Yin</a>,
                                    <a href="https://scholar.google.ca/citations?user=8q2ISMIAAAAJ&hl=en">Daiqing
                                        Li</a>,
                                    <a href="https://orlitany.github.io/">Or Litany</a>,
                                    <a href="https://zgojcic.github.io/">Zan Gojcic</a>,
                                    <a href="http://www.cs.utoronto.ca/~fidler/">Sanja Fidler</a>
                                    <br>
                                    <em>NeurIPS</em>, 2022
                                    <font color="red"><strong> &nbsp (Spotlight Presentation)</strong></font>
                                    <br>
                                    <a href="https://nv-tlabs.github.io/GET3D/">project page</a> /
                                    <a href="https://arxiv.org/abs/2209.11163">arXiv</a> /
                                    <a href="https://github.com/nv-tlabs/GET3D">code</a> /
                                    <a href="https://www.youtube.com/watch?v=shy51E-MU8Y">video</a> /
                                    <a href="https://nv-tlabs.github.io/GET3D/assets/bib.txt">bibtex</a> /
                                    <a href="https://youtu.be/shy51E-MU8Y" style="color:#FF0000;">Two Minute Paper</a>
                                    <p>We develop a 3D generative model to generate meshes with textures, bridging the
                                        success in the
                                        differentiable surface modeling, differentiable rendering and 2D GANs.
                                    </p>
                                </td>
                            </tr>
                            <tr onmouseout="drivear_stop()" onmouseover="drivear_start()">
                                <td style="padding: 20px; width: 25%; vertical-align: middle">
                                    <div class="one">
                                        <div class="two" id='drivear_image'>
                                            <img src='./images/ECCV22_DriveAR/2462_b.jpeg' width="100%">
                                        </div>
                                        <img src='./images/ECCV22_DriveAR/2462.jpeg' width="100%">
                                    </div>
                                    <script type="text/javascript">
                                        function drivear_start() {
                                            document.getElementById('drivear_image').style.opacity = "1";
                                        }

                                        function drivear_stop() {
                                            document.getElementById('drivear_image').style.opacity = "0";
                                        }
                                        drivear_stop()
                                    </script>
                                </td>
                                <td style="padding: 20px; width: 75%; vertical-align: top">
                                    <a href="https://nv-tlabs.github.io/outdoor-ar/">
                                        <papertitle>Neural Light Field Estimation for Street Scenes with Differentiable
                                            Virtual Object
                                            Insertion</papertitle>
                                    </a>
                                    <br>
                                    <a href="http://www.cs.toronto.edu/~zianwang/">Zian Wang</a>,
                                    <strong>Wenzheng Chen</strong>,
                                    <a href="http://www.cs.toronto.edu/~davidj/">David Acuna</a>,
                                    <a href="https://jankautz.com">Jan Kautz</a>,
                                    <a href="https://www.cs.utoronto.ca/~fidler/">Sanja Fidler</a>
                                    <br>
                                    <em>ECCV</em>, 2022
                                    <br>
                                    <a href="https://nv-tlabs.github.io/outdoor-ar/">project page</a> /
                                    <a href="https://arxiv.org/abs/2208.09480">arXiv</a> /
                                    code /
                                    <a href="https://youtu.be/OnhYDC_rygQ">video</a> /
                                    <a href="https://nv-tlabs.github.io/outdoor-ar/assets/outdoor22_bib.txt">bibtex</a>
                                    <p> We propose a hybrid lighting representation to represent spatial-varying
                                        lighting for complex
                                        outdoor street scenes.
                                    </p>
                                </td>
                            </tr>
                            <tr onmouseout="jointopt_stop()" onmouseover="jointopt_start()">

                                <td style="padding:20px;width:25%;vertical-align: middle">
                                    <img src='./images/jointopt/jointopt.jpg' width="100%">
                                </td>

                                <td style="padding:20px;width:75%;vertical-align: top">
                                    <a href="https://nvlabs.github.io/nvdiffrec/">
                                        <papertitle>Extracting Triangular 3D Models, Materials, and Lighting From Images
                                        </papertitle>
                                    </a>
                                    <br>
                                    <a href="https://research.nvidia.com/person/jacob-munkberg">Jacob Munkberg</a>,
                                    <a href="https://research.nvidia.com/person/jon-hasselgren">Jon Hasselgren</a>,
                                    <a href="http://www.cs.toronto.edu/~shenti11/">Tianchang Shen</a>,
                                    <a href="http://www.cs.toronto.edu/~jungao/">Jun Gao</a>,
                                    <strong>Wenzheng Chen</strong>,
                                    <a href="https://research.nvidia.com/person/alex-evans">Alex Evans</a>,
                                    <a href="https://research.nvidia.com/person/thomas-mueller">Thomas M√ºller</a>,
                                    <a href="http://www.cs.utoronto.ca/~fidler/">Sanja Fidler</a>
                                    <br>
                                    <em>CVPR</em>, 2022 &nbsp
                                    <font color="red"><strong> (Oral Presentation)</strong></font>
                                    <br>
                                    <a href="https://nvlabs.github.io/nvdiffrec/">project page</a> /
                                    <a href="https://arxiv.org/abs/2111.12503">arXiv</a> /
                                    <a href="https://github.com/NVlabs/nvdiffrec">code</a> /
                                    <a href="https://www.youtube.com/watch?v=5j8I7V6blqM">video</a> /
                                    <a href="https://nvlabs.github.io/nvdiffrec/assets/bib.txt">bibtex</a> /
                                    <a href="https://youtu.be/5j8I7V6blqM" style="color:#FF0000;">Two Minute Paper</a>
                                    <p>Nvdiffrec reconstructs 3D mesh with materials from multi-view images by combining
                                        diff surface
                                        modeling with diff renderer. The method supports Nvidia <a
                                            href="https://www.youtube.com/watch?v=vgot-CK1xRk">Neural Drivesim</a></p>
                                </td>
                            </tr>
                            <!-- Publication 2021-->
                            <tr bgcolor="#ffffd0">
                                <td style="padding: 20px; width: 25%; vertical-align: middle">
                                    <img src='./images/dibrpp/dibrpp_intro.jpeg' width="100%">
                                </td>
                                <td style="padding: 20px; width: 75%; vertical-align: middle">
                                    <a href="https://nv-tlabs.github.io/DIBRPlus/">
                                        <papertitle>DIB-R++: Learning to Predict Lighting and Material with a Hybrid
                                            Differentiable Renderer
                                        </papertitle>
                                    </a>
                                    <br>
                                    <strong>Wenzheng Chen</strong>,
                                    <a href="https://joeylitalien.github.io">Joey Litalien</a>,
                                    <a href="http://www.cs.toronto.edu/~jungao/">Jun Gao</a>,
                                    <a href="http://www.cs.toronto.edu/~zianwang/">Zian Wang</a>,
                                    <a href="https://github.com/caenorst">Clement Fuji Tsang</a>,
                                    <a href="https://www.samehkhamis.com">Sameh Khamis</a>,
                                    <a href="https://orlitany.github.io">Or Litany</a>,
                                    <a href="https://www.cs.utoronto.ca/~fidler/">Sanja Fidler</a>
                                    <br>
                                    <em>NeurIPS</em>, 2021
                                    <br>
                                    <a href="https://nv-tlabs.github.io/DIBRPlus/">project page</a> /
                                    <a href="https://arxiv.org/abs/2111.00140">arXiv</a> /
                                    code /
                                    <a href="https://papertalk.org/papertalks/36945">video</a> /
                                    <a href="https://nv-tlabs.github.io/DIBRPlus/bib.html">bibtex</a>
                                    <p>DIB-R++ is a high-performant differentiable renderer which combines rasterization
                                        and ray-tracing
                                        together and supports advanced lighitng and material effects. We further embed
                                        it in deep learning
                                        and jointly predict geometry, texture, light and material from a single image.
                                    </p>
                                </td>
                            </tr>
                            <tr bgcolor="#ffffd0">
                                <td style="padding: 20px; width: 25%; vertical-align: middle">
                                    <img src="images/20-stylegan3.jpg" width="100%">
                                </td>
                                <td style="padding: 20px; width: 75%; vertical-align: middle">
                                    <a href="https://nv-tlabs.github.io/GANverse3D/">
                                        <papertitle>Image GANs meet Differentiable Rendering for Inverse
                                            Graphics and Interpretable 3D Neural
                                            Rendering</papertitle>
                                    </a>
                                    <br>
                                    <a>Yuxuan Zhang*</a>, <strong>Wenzheng Chen*</strong>,
                                    <a href="http://www.cs.toronto.edu/~jungao/">Jun Gao</a>,
                                    <a href="http://www.cs.toronto.edu/~linghuan/">Huan Ling</a>, <a>Yinan Zhang</a>,
                                    <br>
                                    <a href="https://groups.csail.mit.edu/vision/torralbalab/">Antonio Torralba</a>
                                    <a href="http://www.cs.utoronto.ca/~fidler/">Sanja Fidler</a>
                                    (* Equal contribution)
                                    <br>
                                    <em>ICLR</em>, 2021 &nbsp
                                    <font color="red"><strong>(Oral Presentation)</strong></font>
                                    <br>
                                    <br>
                                    <a href="https://nv-tlabs.github.io/GANverse3D/">project page</a> /
                                    <a href="https://arxiv.org/abs/2010.09125">arXiv</a>
                                    / code /
                                    <a href="https://papertalk.org/papertalks/28644">video</a> /
                                    <a href="papers/20-stylerender.bib">bibtex</a>
                                    <p></p>
                                    <p>
                                        We explore StyleGAN as a multi-view image generator and
                                        train inverse graphics from StyleGAN images. Once trained,
                                        the invere graphics model further helps disentangle and
                                        manipulate StyleGAN latent code from graphics
                                        knowledge. Our work was featured at<a href="https://youtu.be/eAn_oiZwUXA?t=4705"
                                            style="color:#FF0000;"> NVIDIA GTC 2021</a> and has become an <a
                                            href="https://omniverse-content-production.s3-us-west-2.amazonaws.com/Assets/Isaac/Documentation/Isaac-Sim-Docs_2022.2.1/extensions/latest/ext_ganverse3d.html">Omniverse
                                            product</a>.
                                    </p>
                                </td>
                            </tr>
                            <!-- Publication 2020-->
                            <tr bgcolor="#ffffd0" onmouseout="dpzlearn_stop()" onmouseover="dpzlearn_start()">
                                <td style="padding: 20px; width: 25%; vertical-align: middle">
                                    <img width="90%" src="images/20-nlos3.png" />
                                </td>
                                <td style="padding: 20px; width: 75%; vertical-align: top">
                                    <a href="https://light.cs.princeton.edu/publication/nlos-learnedfeatures/">
                                        <papertitle>Learned Feature Embeddings for Non-Line-of-Sight
                                            Imaging and Recognition</papertitle>
                                    </a>
                                    <br />
                                    <strong>Wenzheng Chen*</strong>,
                                    <a href="https://weify627.github.io/">Fangyin Wei*</a>,
                                    <a href="http://www.cs.toronto.edu/~kyros/">Kyros Kutulakos</a>,
                                    <br />
                                    <a href="https://www.cs.princeton.edu/~smr/">Szymon Rusinkiewicz</a>,
                                    <a href="http://www.cs.princeton.edu/~fheide/">Felix Heide
                                    </a>
                                    (* Equal contribution)
                                    <br />
                                    <em>SIGGRAPH Asia</em>, 2020 &nbsp
                                    <font color="red"><strong></strong></font>
                                    <br />
                                    <a href="https://light.cs.princeton.edu/publication/nlos-learnedfeatures/">
                                        project page</a> /
                                    <a
                                        href="https://light.cs.princeton.edu/wp-content/uploads/2020/09/NLOS-LearnedFeatures-main.pdf">paper</a>
                                    / <a
                                        href="https://github.com/princeton-computational-imaging/NLOSFeatureEmbeddings">code</a>
                                    /
                                    <a href="papers/20-nlos.bib">bibtex</a> /
                                    <a href="https://artofsci.princeton.edu/2023-digital-exhibition/"
                                        style="color:#FF0000;">Art of Science Exhibition</a>
                                    <p></p>
                                    <p>
                                        We propose to learn feature embeddings for
                                        non-line-of-sight imaging and recognition by propagating
                                        features through physical modules.
                                    </p>
                                </td>
                            </tr>
                            <tr onmouseout="dpzlearn_stop()" onmouseover="dpzlearn_start()">
                                <td style="padding: 20px; width: 25%; vertical-align: middle">
                                    <img src="images/20-diftet.jpg" width="100%">
                                </td>
                                <td style="padding: 20px; width: 75%; vertical-align: middle">
                                    <a href="https://nv-tlabs.github.io/DefTet/">
                                        <papertitle>Learning Deformable Tetrahedral Meshes for 3D
                                            Reconstruction</papertitle>
                                    </a>
                                    <br>
                                    <a href="http://www.cs.toronto.edu/~jungao/">Jun Gao</a>,
                                    <strong>Wenzheng Chen</strong>, <a>Tommy Xiang</a>,
                                    <a href="https://www.cs.toronto.edu/~jacobson/">Alec Jacobson</a>,
                                    <a href="https://www.cs.williams.edu/~morgan/">Morgan Mcguire</a>,
                                    <a href="http://www.cs.utoronto.ca/~fidler/">Sanja Fidler</a>
                                    <br>
                                    <em>NeurIPS</em>, 2020 &nbsp
                                    <font color="red"><strong></strong></font>
                                    <br>
                                    <a href="https://nv-tlabs.github.io/DefTet/">project page</a> /
                                    <a>arXiv</a>
                                    /
                                    <a href="https://github.com/nv-tlabs/DefTet">code</a>
                                    /
                                    <a href="https://papertalk.org/papertalks/9550">video</a> /
                                    <a href="papers/20-deftet.bib">bibtex</a>
                                    <p></p>
                                    <p>
                                        We predict deformable tetrahedral meshes from images or
                                        point clouds, which support arbitrary topologies. We also
                                        design a differentiable renderer for tetrahedron, allowing
                                        3D reconstrucion from 2D supervison only.
                                    </p>
                                </td>
                            </tr>
                            <tr bgcolor="#ffffd0" onmouseout="dpzlearn_stop()" onmouseover="dpzlearn_start()">
                                <td style="padding: 20px; width: 25%; vertical-align: middle">
                                    <img width="90%" src="images/20-autotunning.gif" />
                                </td>
                                <td style="padding: 20px; width: 75%; vertical-align: middle">
                                    <a href="https://www.dgp.toronto.edu/autotuningsl/">
                                        <papertitle>Auto-Tuning Structured Light by Optical Stochastic
                                            Gradient Descent</papertitle>
                                    </a>
                                    <br />
                                    <strong>Wenzheng Chen*</strong>,
                                    <a href="https://www.cs.toronto.edu/~parsa/">Parsa Mirdehghan*</a>,
                                    <a href="http://www.cs.utoronto.ca/~fidler/">Sanja Fidler</a>,
                                    <a href="https://www.cs.toronto.edu/~kyros/">Kyros Kutulakos</a>
                                    (* Equal contribution)
                                    <br />
                                    <em>CVPR</em>, 2020 &nbsp
                                    <font color="red"><strong>(ICCP 2021 Best Poster Award)</strong></font>
                                    <font color="red"><strong></strong></font>
                                    <br />
                                    <a href="https://www.dgp.toronto.edu/autotuningsl/">
                                        project page</a> /
                                    <a href="https://www.dgp.toronto.edu/autotuningsl/ims/7187.pdf">paper</a> /
                                    <a href="https://www.dgp.toronto.edu/autotuningsl/">code</a> /
                                    <a href="https://papertalk.org/papertalks/14703">video</a> /
                                    <a href="papers/20-autotuning.bib">bibtex</a>
                                    <p></p>
                                    <p>
                                        We present optical SGD, a computational imaging technique
                                        that allows an active depth imaging system to
                                        automatically discover optimal illuminations & decoding.
                                    </p>
                                </td>
                            </tr>
                            <!-- Publication 2019-->
                            <tr bgcolor="#ffffd0" onmouseout="dpzlearn_stop()" onmouseover="dpzlearn_start()">
                                <td style="padding: 20px; width: 25%; vertical-align: middle">
                                    <img width="100%" src="images/dibrender.jpg" />
                                </td>
                                <td style="padding: 20px; width: 75%; vertical-align: top">
                                    <a href="https://research.nvidia.com/labs/toronto-ai/DIB-R/">
                                        <papertitle>Learning to Predict 3D Objects with an
                                            Interpolation-based Differentiable Renderer</papertitle>
                                    </a>
                                    <br />
                                    <strong>Wenzheng Chen</strong>,
                                    <a href="http://www.cs.toronto.edu/~jungao/">Jun Gao*</a>,
                                    <a href="http://www.cs.toronto.edu/~linghuan/">Huan Ling*</a>, <a>Edward J.
                                        Smith*</a>,
                                    <br />
                                    <a href="https://users.aalto.fi/~lehtinj7/">Jaakko Lehtinen</a>,
                                    <a href="https://www.cs.toronto.edu/~jacobson/">Alec Jacobson</a>,
                                    <a href="http://www.cs.utoronto.ca/~fidler/">Sanja Fidler</a>
                                    (* Equal contribution)
                                    <br />
                                    <em>NeurIPS</em>, 2019 &nbsp
                                    <font color="red"><strong></strong></font>
                                    <br />
                                    <a href="https://nv-tlabs.github.io/DIB-R/">project page</a> /
                                    <a href="https://arxiv.org/abs/1908.01210">arXiv</a>
                                    /
                                    <a href="https://github.com/nv-tlabs/DIB-R">code</a>
                                    /
                                    <a href="papers/19-dib.bib">bibtex</a>
                                    /
                                    <a href="https://youtu.be/548sCh0mMRc" style="color:#FF0000;">Two Minute Paper</a>
                                    <p></p>
                                    <p>
                                        An interpolation-based 3D mesh differentiable renderer
                                        that supports vertex, vertex color, multiple lighting
                                        models, texture mapping and could be easily embedded in
                                        neural networks.
                                    </p>
                                </td>
                            </tr>
                            <tr bgcolor="#ffffd0" onmouseout="dpzlearn_stop()" onmouseover="dpzlearn_start()">
                                <td style="padding: 20px; width: 25%; vertical-align: middle">
                                    <img width="100%" src="images/19-nlos.png" />
                                </td>
                                <td style="padding: 20px; width: 75%; vertical-align: middle">
                                    <a href="https://www.cs.princeton.edu/~fheide/steadystatenlos">
                                        <papertitle>Steady-state Non-Line-of-Sight Imaging</papertitle>
                                    </a>
                                    <br />
                                    <strong>Wenzheng Chen</strong>, <a>Simon Daneau</a>,
                                    <a>Fahim Mannan</a>,
                                    <a href="https://www.cs.princeton.edu/~fheide/">Felix Heide</a>
                                    <br />
                                    <em>CVPR</em>, 2019 &nbsp
                                    <font color="red"><strong>(Oral Presentation)</strong></font>
                                    <br />
                                    <a href="https://www.cs.princeton.edu/~fheide/steadystatenlos">
                                        project page</a> /
                                    <a href="https://arxiv.org/abs/1811.09910">arXiv</a>
                                    /
                                    <a
                                        href="https://github.com/wenzhengchen/Steady-state-Non-Line-of-Sight-Imaging">code</a>
                                    /
                                    <a href="papers/19-nlos.bib">bibtex</a>
                                    <p></p>
                                    <p>We show hidden objects can be recovereed from conventional images instead of
                                        transient images.</p>
                                </td>
                            </tr>
                            <tr onmouseout="dpzlearn_stop()" onmouseover="dpzlearn_start()">
                                <td style="padding: 20px; width: 25%; vertical-align: middle">
                                    <img width="100%" src="images/19-gcn.jpg" />
                                </td>
                                <td style="padding: 20px; width: 75%; vertical-align: top">
                                    <a href="https://github.com/fidler-lab/curve-gcn">
                                        <papertitle>Fast Interactive Object Annotation with
                                            Curve-GCN</papertitle>
                                    </a>
                                    <br />
                                    <a href="http://www.cs.toronto.edu/~linghuan/">Huan Ling*</a>,
                                    <a href="http://www.cs.toronto.edu/~jungao/">Jun Gao*</a>,
                                    <a href="https://amlankar.github.io/">Amlan Kar</a>,
                                    <strong>Wenzheng Chen</strong>,
                                    <a href="http://www.cs.utoronto.ca/~fidler/">Sanja Fidler</a>
                                    (* Equal contribution)
                                    <br />
                                    <em>CVPR</em>, 2019 &nbsp
                                    <font color="red"><strong></strong></font>
                                    <br />
                                    <a href="https://github.com/fidler-lab/curve-gcn">project page</a>
                                    /
                                    <a href="https://arxiv.org/abs/1903.06874">arXiv</a>
                                    /
                                    <a href="https://github.com/fidler-lab/curve-gcn">code</a>
                                    /
                                    <a href="papers/19-gcn.bib">bibtex</a>
                                    <p></p>
                                    <p>
                                        We predict object polygon contours from graph neural
                                        networks, where a novel 2D differentiable rendering loss is
                                        introduced. It renders a polygon countour into a segmentation mask
                                        and back propagates the loss to help optimize the polygon
                                        vertices.
                                    </p>
                                </td>
                            </tr>
                            <!-- Publication 2018-->
                            <tr onmouseout="dpzlearn_stop()" onmouseover="dpzlearn_start()">
                                <td style="padding: 20px; width: 25%; vertical-align: middle">
                                    <img width="100%" src="images/18-optimalsl.png" />
                                </td>
                                <td style="padding: 20px; width: 75%; vertical-align: middle">
                                    <a href="http://www.dgp.toronto.edu/optimalsl/">
                                        <papertitle>Optimal Structured Light a la Carte</papertitle>
                                    </a>
                                    <br>
                                    <a href="http://www.cs.toronto.edu/~parsa/">Parsa Mirdehghan</a>, <strong>Wenzheng
                                        Chen</strong>,
                                    <a href="hhttps://www.cs.toronto.edu/~kyros/">Kyros Kutulakos</a>
                                    <br />
                                    <em>CVPR</em>, 2018 &nbsp
                                    <font color="red"><strong>(Spotlight Presentation)</strong></font>
                                    <br>
                                    <a href="http://www.dgp.toronto.edu/optimalsl/">
                                        project page</a> /
                                    <a href="http://www.dgp.toronto.edu/optimalsl/files/1697.pdf">paper</a>
                                    /
                                    <a href="http://www.dgp.toronto.edu/optimalsl/">code</a>
                                    /
                                    <a href="papers/18-sl.bib">bibtex</a>
                                    <p></p>
                                    <p>
                                        alacarte designs structured light patterns from a maching
                                        learning persepctive, where patterns are automatically
                                        optimized by minimizing the disparity error under any given imaging condition.
                                    </p>
                                </td>
                            </tr>
                            <!-- Publication 2016-->
                            <tr bgcolor="#ffffd0" onmouseout="d3d_stop()" onmouseover="d3d_start()">
                                <td style="padding: 20px; width: 25%; vertical-align: middle" align="center">
                                    <div class="one">
                                        <div class="two" id="d3d_image">
                                            <img style="vertical-align: middle" height="100%"
                                                src="images/d3d_after.jpg" />
                                        </div>
                                        <img style="vertical-align: middle" height="100%" src="images/d3d_before.jpg" />
                                    </div>
                                    <script type="text/javascript">
                                        function d3d_start() {
                                            document.getElementById("d3d_image").style.opacity =
                                                "1";
                                        }

                                        function d3d_stop() {
                                            document.getElementById("d3d_image").style.opacity =
                                                "0";
                                        }
                                        d3d_stop();
                                    </script>
                                </td>
                                <td style="padding: 20px; width: 75%; vertical-align: middle">
                                    <a href="http://irc.cs.sdu.edu.cn/Deep3DPose/">
                                        <papertitle>Synthesizing Training Images for Boosting Human 3D Pose
                                            Estimation</papertitle>
                                    </a>
                                    <br />
                                    <strong>Wenzheng Chen</strong>, <a>Huan Wang</a>,
                                    <a href="http://yangyan.li/">Yangyan Li</a>,
                                    <a href="https://cseweb.ucsd.edu/~haosu/">Hao Su</a>,
                                    <a>Zhenhua Wang</a>,
                                    <br />
                                    <a href="http://irc.cs.sdu.edu.cn/~chtu/index.html">Changhe Tu</a>,
                                    <a href="http://www.cs.huji.ac.il/~danix/">Dani Lischinski</a>,
                                    <a href="http://www.math.tau.ac.il/~dcor/">Daniel Cohen-Or</a>,
                                    <a href="https://cfcs.pku.edu.cn/baoquan/">Baoquan Chen</a>
                                    <br />
                                    <em>3DV</em>, 2016 &nbsp
                                    <font color="red"><strong>(Oral Presentation)</strong></font>
                                    <br />
                                    <a href="http://irc.cs.sdu.edu.cn/Deep3DPose/">project page</a> /
                                    <a href="https://arxiv.org/abs/1604.02703">arXiv</a>
                                    /
                                    <a href="https://github.com/chen1474147/Deep3DPose">code</a>
                                    /
                                    <a href="papers/16-3dv.bib">bibtex</a>
                                    <p></p>
                                    <p>
                                        3D pose estimation from model trained with synthetic data
                                        and domain adaptation.
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>


                    <!--  last part -->
                    <table
                        style="width: 100%; border: 0px; border-spacing: 0px; border-collapse: separate; margin-right: auto; margin-left: auto;">
                        <tbody>
                            <tr>
                                <td style="padding: 0px">
                                    <br />
                                    <div style="width: 30%; max-width: 150px; margin: 0 auto;">
                                        <script type="text/javascript" id="clstr_globe"
                                            src="https://clustrmaps.com/globe.js?d=XCSBU29m-A_ElmMcdtWVGep9CwU2wM-Lha6Alp5WOwQ"></script>
                                    </div>
                                    <br />
                                    <p style="text-align: center; font-size: small;">
                                        Template adapted from <a
                                            href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                </td>
            </tr>
        </tbody>
    </table>

    <script>

        function toggleNews() {
            var moreNewsDiv = document.getElementById('moreNews');
            var btn = document.getElementById('toggleNewsBtn');

            // Check if the news is hidden
            if (moreNewsDiv.style.display === 'none' || moreNewsDiv.style.display === '') {
                // If hidden, show it and change button text
                moreNewsDiv.style.display = 'block';
                btn.innerHTML = '[ less ]';
            } else {
                // If shown, hide it and change button text
                moreNewsDiv.style.display = 'none';
                btn.innerHTML = '[ more... ]';
            }
        }
    </script>

</body>

</html>